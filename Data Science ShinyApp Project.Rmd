---
title: "Data Science_Capstone_ShinyApp"
author: "Jun Nelson"
date: "1/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction of project
The goal of this exercise is to create a product to highlight the prediction algorithm that you have built and to provide an interface that can be accessed by others. For this project you must submit:

A Shiny app that takes as input a phrase (multiple words) in a text box input and outputs a prediction of the next word.
A slide deck consisting of no more than 5 slides created with R Studio Presenter (https://support.rstudio.com/hc/en-us/articles/200486468-Authoring-R-Presentations) pitching your algorithm and app as if you were presenting to your boss or an investor.

## ShinyApp project
1. Create a data product to show off your prediction algorithm You should create a Shiny app that accepts an n-gram and predicts the next word using previous ones. It is like that used in the Smartphone keyboard App by SWIFTKEY.
2. Previous tasks focused on exploration of data. You should get to know the data and understand the problem. Manipulate data (cleaning and modeling). Create prediction model and execute data and information into ShinyApp. 
3.Document the use of your data product so that others can rapidly deploy your algorithm
4.Consider the size of the predictive model you have developed. You may have to sacrifice some accuracy to have a fast enough/small enough model to load into Shiny

## Slide Deck
1. The Shiny Application enable end users to predict three possible words of a sentence.
2. The predicted words has been retrieved from the N-Gram tables by comparing it with tokenized frequency of 2,3,and 4 Grams. It will show predicted words. 
3. An user entered the text/phrase in a input box. Another box will show the written phrase/word
4. "Submit" button allows users to observe the predicted words in a control way after entering the phrase/words. 


## Download and load dataset into R
* The data was loaded from Coursera Link (https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)  to local PC
*setwd("./Coursera-SwiftKey/final/en_US")
## Read datasets


``` {r}
news <- file("C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/en_US.news.txt", open="r")
    news_text <- readLines(news); close(news)
    
    blogs <- file("C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/en_US.blogs.txt", open="r")
    blogs_text <- readLines(blogs); close(blogs) 

    twit<- file("C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/en_US.twitter.txt", open="r")
    Twit_text <- readLines(twit); close(twit)
``` 


``` {r}
library(ggplot2)
library(plyr)
library(magrittr)
library(SnowballC)
library(stringr)
library(stringi)
library(tm)
library(tokenizers)
library(quanteda)
library(rJava)
library(RWeka)
library(parallel)
library(R.utils)
library(dplyr)
library(shiny)
library(NLP)
```


## Data Preparation
*Sample the data and create the corpus

```{r}
subBlogs <- sample(blogs_text, size = 1000)
subNews <- sample(news_text, size = 1000)
subTwitter <- sample(Twit_text, size = 1000)
sampledData <- c(subBlogs, subNews, subTwitter)
#corpus <- VCorpus(VectorSource(sampledData))
``` 

* write sampled texts into text files for further analysis

``` {r}
writeLines(sampledData, "C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/Capstone_ShinyApp/sampledData.txt")
```


## Clean Corpus (Remove punctuation, stopwords, whitespaces, numbers etc.) 

```{r}
cleansing <- function (textcp) {
  textcp <- tm_map(textcp, content_transformer(tolower))
  textcp <- tm_map(textcp, stripWhitespace)
  textcp <- tm_map(textcp, removePunctuation)
  textcp <- tm_map(textcp, removeNumbers)
  textcp
}

sampledData <- VCorpus(DirSource("C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/Capstone_Shinyapp", encoding = "UTF-8"))
```


* Tokenize sampled text data

```{r}
sampledData <- cleansing(sampledData)
```

## Create TermDocumentMatrix 
* Define function to make Ngrams

```{r}
tdm_Ngram <- function (textcp, n) {
  NgramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = n, max = n))}
  tdm_ngram <- TermDocumentMatrix(textcp, control = list(tokenizer = NgramTokenizer))
  tdm_ngram
}
```

* Define function to extract the N grams and sort

```{r}
ngram_sorted_df <- function (tdm_ngram) {
  tdm_ngram_m <- as.matrix(tdm_ngram)
  tdm_ngram_df <- as.data.frame(tdm_ngram_m)
  colnames(tdm_ngram_df) <- "Count"
  tdm_ngram_df <- tdm_ngram_df[order(-tdm_ngram_df$Count), , drop = FALSE]
  tdm_ngram_df
}
```

* Catogrize NGrams

```{r}
tdm_1gram <- tdm_Ngram(sampledData, 1)
tdm_2gram <- tdm_Ngram(sampledData, 2)
tdm_3gram <- tdm_Ngram(sampledData, 3)
tdm_4gram <- tdm_Ngram(sampledData, 4)
```


* Make NGrams tables from NGrams

```{r}
tdm_1gram_df <- ngram_sorted_df(tdm_1gram)
tdm_2gram_df <- ngram_sorted_df(tdm_2gram)
tdm_3gram_df <- ngram_sorted_df(tdm_3gram)
tdm_4gram_df <- ngram_sorted_df(tdm_4gram)
```

## # Creat r-compressed files from data frame
* 4Gram file

```{r}
quadgram <- data.frame(rows=rownames(tdm_4gram_df),count=tdm_4gram_df$Count)
quadgram$rows <- as.character(quadgram$rows)
quadgram_split <- strsplit(as.character(quadgram$rows),split=" ")
quadgram <- transform(quadgram,first = sapply(quadgram_split,"[[",1),second = sapply(quadgram_split,"[[",2),third = sapply(quadgram_split,"[[",3), fourth = sapply(quadgram_split,"[[",4))
quadgram <- data.frame(unigram = quadgram$first,bigram = quadgram$second, trigram = quadgram$third, quadgram = quadgram$fourth, freq = quadgram$count,stringsAsFactors=FALSE)
write.csv(quadgram[quadgram$freq > 1,],"C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/Capstone_ShinyApp/quadgram.csv",row.names=F)
quadgram <- read.csv("C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/Capstone_ShinyApp/quadgram.csv",stringsAsFactors = F)
saveRDS(quadgram,"C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/Capstone_ShinyApp/quadgram.RData")
```

 *3Gram file
 
```{r}
 trigram <- data.frame(rows=rownames(tdm_3gram_df),count=tdm_3gram_df$Count)
trigram$rows <- as.character(trigram$rows)
trigram_split <- strsplit(as.character(trigram$rows),split=" ")
trigram <- transform(trigram,first = sapply(trigram_split,"[[",1),second = sapply(trigram_split,"[[",2),third = sapply(trigram_split,"[[",3))
trigram <- data.frame(unigram = trigram$first,bigram = trigram$second, trigram = trigram$third, freq = trigram$count,stringsAsFactors=FALSE)
write.csv(trigram[trigram$freq > 1,],"C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/Capstone_ShinyApp/trigram.csv",row.names=F)
trigram <- read.csv("C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/Capstone_ShinyApp/trigram.csv",stringsAsFactors = F)
saveRDS(trigram,"C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/Capstone_ShinyApp/trigram.RData")
```


*2Gram file

```{r}
bigram <- data.frame(rows=rownames(tdm_2gram_df),count=tdm_2gram_df$Count)
bigram$rows <- as.character(bigram$rows)
bigram_split <- strsplit(as.character(bigram$rows),split=" ")
bigram <- transform(bigram,first = sapply(bigram_split,"[[",1),second = sapply(bigram_split,"[[",2))
bigram <- data.frame(unigram = bigram$first,bigram = bigram$second,freq = bigram$count,stringsAsFactors=FALSE)
write.csv(bigram[bigram$freq > 1,],"C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/Capstone_ShinyApp/bigram.csv",row.names=F)
bigram <- read.csv("C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/Capstone_ShinyApp/bigram.csv",stringsAsFactors = F)
saveRDS(bigram,"C:/Users/HP/Desktop/Grace R/Course 10/Coursera-SwiftKey/final/en_US/Capstone_ShinyApp/bigram.RData")
```

## Plan to Make shinyApp
-Input a phrase (multiple words) in a text box input and outputs a prediction of the next word
## Make a Slide Deck with R Studio Presenter
-Consist of no more than 5 slides
